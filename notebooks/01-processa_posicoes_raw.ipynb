{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_server = \"spark://spark-master-otmzsp:7077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a sessão do Spark com as dependências e variáveis do S3A\n",
    "# .master(spark_server) \\\n",
    "# .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Processa Posições Raw\") \\\n",
    "    .master(spark_server) \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-otmzsp:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://df0f425e685d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master-otmzsp:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Processa Posições Raw</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7818b032ce90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 's3a://raw/posicoes'\n",
    "trusted = 's3a://trusted/posicoes'\n",
    "checkpoint = 's3a://trusted/posicoes_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, TimestampType, FloatType\n",
    "\n",
    "# Defining the schema for the JSON\n",
    "schema = StructType([StructField(\"hr\", StringType(), True),\n",
    "                     StructField(\"l\", ArrayType(StructType([StructField(\"c\", StringType(), True),\n",
    "                                                            StructField(\"cl\", IntegerType(), True),\n",
    "                                                            StructField(\"sl\", IntegerType(), True),\n",
    "                                                            StructField(\"lt0\", StringType(), True),\n",
    "                                                            StructField(\"lt1\", StringType(), True),\n",
    "                                                            StructField(\"qv\", IntegerType(), True),\n",
    "                                                            StructField(\"vs\", ArrayType(StructType([StructField(\"p\", IntegerType(), True),\n",
    "                                                                                                    StructField(\"a\", BooleanType(), True),\n",
    "                                                                                                    StructField(\"ta\", TimestampType(), True),\n",
    "                                                                                                    StructField(\"py\", FloatType(), True),\n",
    "                                                                                                    StructField(\"px\", FloatType(), True),\n",
    "                                                                                                    StructField(\"sv\", StringType(), True),\n",
    "                                                                                                    StructField(\"is\", StringType(), True)\n",
    "                                                                                                    ])\n",
    "                                                                                        )\n",
    "                                                                        )\n",
    "                                                            ])\n",
    "                                                )\n",
    "                                )\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.schema(schema).json(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura incremental de arquivos JSON da camada raw\n",
    "df_stream = spark.readStream.format(\"json\").schema(schema).load(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, sha1\n",
    "from pyspark.sql.functions import (\n",
    "    explode, col, to_timestamp, when, from_utc_timestamp, to_date\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Assumindo que 'df' é o DataFrame original carregado do JSON da SPTrans\n",
    "\n",
    "df_final = df_stream.select(\n",
    "    to_timestamp(col(\"hr\")).alias(\"veiculo_horario_referencia\"),\n",
    "    explode(\"l\").alias(\"linha\")\n",
    ").select(\n",
    "    col(\"veiculo_horario_referencia\"),\n",
    "    col(\"linha.c\").alias(\"veiculo_letreiro_completo\"),\n",
    "    col(\"linha.cl\").alias(\"veiculo_linha_codigo\"),\n",
    "    col(\"linha.sl\").alias(\"veiculo_sentido\"),\n",
    "    col(\"linha.lt0\").alias(\"veiculo_letreiro_destino\"),\n",
    "    col(\"linha.lt1\").alias(\"veiculo_letreiro_origem\"),\n",
    "    col(\"linha.qv\").alias(\"qtde_veiculos_linha\"),\n",
    "    explode(\"linha.vs\").alias(\"veiculo\")\n",
    ").select(\n",
    "    \"*\",\n",
    "    col(\"veiculo.p\").alias(\"veiculo_prefixo\"),\n",
    "    col(\"veiculo.a\").alias(\"veiculo_acessibilidade\"),\n",
    "    col(\"veiculo.ta\").alias(\"veiculo_horario_utc_captura\"),\n",
    "    col(\"veiculo.py\").cast(DoubleType()).alias(\"veiculo_latitude\"),\n",
    "    col(\"veiculo.px\").cast(DoubleType()).alias(\"veiculo_longitude\")\n",
    ").drop(\"linha\", \"veiculo\")\n",
    "\n",
    "# Tratamento do sentido do veículo\n",
    "df_final = df_final.withColumn(\n",
    "    \"veiculo_sentido\",\n",
    "    when(col(\"veiculo_sentido\") == 1, \"Bairro\")\n",
    "    .when(col(\"veiculo_sentido\") == 2, \"Centro\")\n",
    "    .otherwise(col(\"veiculo_sentido\"))\n",
    ")\n",
    "\n",
    "# Tratamento da acessibilidade\n",
    "df_final = df_final.withColumn(\n",
    "    \"veiculo_acessibilidade\",\n",
    "    when(col(\"veiculo_acessibilidade\") == \"true\", \"Acessível\")\n",
    "    .when(col(\"veiculo_acessibilidade\") == \"false\", \"Não acessível\")\n",
    "    .otherwise(\"Informação não disponível\")\n",
    ")\n",
    "\n",
    "# Converter horário UTC para horário local (São Paulo, UTC-3)\n",
    "df_final = df_final.withColumn(\n",
    "    \"veiculo_horario_local_captura\",\n",
    "    from_utc_timestamp(to_timestamp(col(\"veiculo_horario_utc_captura\")), \"America/Sao_Paulo\")\n",
    ")\n",
    "\n",
    "# Adicionar coluna com o tipo de operação da linha\n",
    "df_final = df_final.withColumn(\n",
    "    \"tipo_operacao_linha\",\n",
    "    when(col(\"veiculo_letreiro_completo\").substr(-2, 2) == \"10\", \"Linha Base\")\n",
    "    .when(col(\"veiculo_letreiro_completo\").substr(-2, 2).isin(\"21\", \"23\", \"32\", \"41\"), \"Linha de Atendimento\")\n",
    "    .otherwise(\"Outro tipo de operação\")\n",
    ")\n",
    "\n",
    "# Criar uma coluna 'id' com um identificador único usando SHA-256\n",
    "df_final = df_final.withColumn(\n",
    "    \"id\",\n",
    "    sha1(\n",
    "        concat_ws(\n",
    "            \"-\", \n",
    "            col(\"veiculo_letreiro_completo\"),\n",
    "            col(\"veiculo_linha_codigo\"),\n",
    "            col(\"veiculo_prefixo\"),\n",
    "            col(\"veiculo_horario_local_captura\"),\n",
    "            col(\"veiculo_latitude\"),\n",
    "            col(\"veiculo_longitude\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adicionar a coluna 'datepartition' para particionar por data\n",
    "df_final = df_final.withColumn(\n",
    "    \"datepartition\", to_date(col(\"veiculo_horario_referencia\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-20\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Define o fuso horário de Brasília (GMT-3)\n",
    "br_tz = pytz.timezone('America/Sao_Paulo')\n",
    "\n",
    "# Obtém a data e hora atual em GMT-3\n",
    "now_gmt3 = datetime.now(br_tz)\n",
    "\n",
    "# Formata a data\n",
    "datepartition = now_gmt3.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(datepartition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Supondo que df_final já está definido e contém uma coluna 'id'\n",
    "\n",
    "# Remover registros duplicados com base no campo 'id'\n",
    "df_final_unique = df_final.dropDuplicates([\"id\"])\n",
    "\n",
    "# Escreve o stream em modo append, onde novos dados serão continuamente adicionados\n",
    "query = df_final_unique.writeStream.format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"datepartition\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint) \\\n",
    "    .start(trusted)\n",
    "\n",
    "# Iniciar a consulta de streaming\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
