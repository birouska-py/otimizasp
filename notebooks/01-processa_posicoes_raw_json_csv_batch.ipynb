{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_server = \"spark://spark-master-otmzsp:7077\"\n",
    "minio_server = \"http://minio-otmzsp:9000\" \n",
    "data_inicio_processamento = \"2024-09-23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a sessão do Spark com as dependências e variáveis do S3A\n",
    "# .master(spark_server) \\\n",
    "# .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Processa Posições Raw\") \\\n",
    "    .master(spark_server) \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", minio_server) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b498eb9c187b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master-otmzsp:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Processa Posições Raw</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7860dd77da10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 's3a://raw/posicoes'\n",
    "trusted = 's3a://trusted/posicoes'\n",
    "checkpoint = 's3a://trusted/posicoes_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, TimestampType, FloatType\n",
    "\n",
    "# Defining the schema for the JSON\n",
    "schema = StructType([StructField(\"hr\", StringType(), True),\n",
    "                     StructField(\"l\", ArrayType(StructType([StructField(\"c\", StringType(), True),\n",
    "                                                            StructField(\"cl\", IntegerType(), True),\n",
    "                                                            StructField(\"sl\", IntegerType(), True),\n",
    "                                                            StructField(\"lt0\", StringType(), True),\n",
    "                                                            StructField(\"lt1\", StringType(), True),\n",
    "                                                            StructField(\"qv\", IntegerType(), True),\n",
    "                                                            StructField(\"vs\", ArrayType(StructType([StructField(\"p\", IntegerType(), True),\n",
    "                                                                                                    StructField(\"a\", BooleanType(), True),\n",
    "                                                                                                    StructField(\"ta\", TimestampType(), True),\n",
    "                                                                                                    StructField(\"py\", FloatType(), True),\n",
    "                                                                                                    StructField(\"px\", FloatType(), True),\n",
    "                                                                                                    StructField(\"sv\", StringType(), True),\n",
    "                                                                                                    StructField(\"is\", StringType(), True)\n",
    "                                                                                                    ])\n",
    "                                                                                        )\n",
    "                                                                        )\n",
    "                                                            ])\n",
    "                                                )\n",
    "                                )\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, sha1\n",
    "from pyspark.sql.functions import (\n",
    "    explode, col, to_timestamp, when, from_utc_timestamp, to_date\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def transform_data(df):\n",
    "    # Assumindo que 'df' é o DataFrame original carregado do JSON da SPTrans\n",
    "    df_final = df.select(\n",
    "        to_timestamp(col(\"hr\")).alias(\"veiculo_horario_referencia\"),\n",
    "        explode(\"l\").alias(\"linha\")\n",
    "    ).select(\n",
    "        col(\"veiculo_horario_referencia\"),\n",
    "        col(\"linha.c\").alias(\"veiculo_letreiro_completo\"),\n",
    "        col(\"linha.cl\").alias(\"veiculo_linha_codigo\"),\n",
    "        col(\"linha.sl\").alias(\"veiculo_sentido\"),\n",
    "        col(\"linha.lt0\").alias(\"veiculo_letreiro_destino\"),\n",
    "        col(\"linha.lt1\").alias(\"veiculo_letreiro_origem\"),\n",
    "        col(\"linha.qv\").alias(\"qtde_veiculos_linha\"),\n",
    "        explode(\"linha.vs\").alias(\"veiculo\")\n",
    "    ).select(\n",
    "        \"*\",\n",
    "        col(\"veiculo.p\").alias(\"veiculo_prefixo\"),\n",
    "        col(\"veiculo.a\").alias(\"veiculo_acessibilidade\"),\n",
    "        col(\"veiculo.ta\").alias(\"veiculo_horario_utc_captura\"),\n",
    "        col(\"veiculo.py\").cast(DoubleType()).alias(\"veiculo_latitude\"),\n",
    "        col(\"veiculo.px\").cast(DoubleType()).alias(\"veiculo_longitude\")\n",
    "    ).drop(\"linha\", \"veiculo\")\n",
    "\n",
    "    # Tratamento do sentido do veículo\n",
    "    df_final = df_final.withColumn(\n",
    "        \"veiculo_sentido\",\n",
    "        when(col(\"veiculo_sentido\") == 1, \"Bairro\")\n",
    "        .when(col(\"veiculo_sentido\") == 2, \"Centro\")\n",
    "        .otherwise(col(\"veiculo_sentido\"))\n",
    "    )\n",
    "\n",
    "    # Tratamento da acessibilidade\n",
    "    df_final = df_final.withColumn(\n",
    "        \"veiculo_acessibilidade\",\n",
    "        when(col(\"veiculo_acessibilidade\") == \"true\", \"Acessível\")\n",
    "        .when(col(\"veiculo_acessibilidade\") == \"false\", \"Não acessível\")\n",
    "        .otherwise(\"Informação não disponível\")\n",
    "    )\n",
    "\n",
    "    # Converter horário UTC para horário local (São Paulo, UTC-3)\n",
    "    df_final = df_final.withColumn(\n",
    "        \"veiculo_horario_local_captura\",\n",
    "        from_utc_timestamp(to_timestamp(col(\"veiculo_horario_utc_captura\")), \"America/Sao_Paulo\")\n",
    "    )\n",
    "\n",
    "    # Adicionar coluna com o tipo de operação da linha\n",
    "    df_final = df_final.withColumn(\n",
    "        \"tipo_operacao_linha\",\n",
    "        when(col(\"veiculo_letreiro_completo\").substr(-2, 2) == \"10\", \"Linha Base\")\n",
    "        .when(col(\"veiculo_letreiro_completo\").substr(-2, 2).isin(\"21\", \"23\", \"32\", \"41\"), \"Linha de Atendimento\")\n",
    "        .otherwise(\"Outro tipo de operação\")\n",
    "    )\n",
    "\n",
    "    # Criar uma coluna 'id' com um identificador único usando SHA-256\n",
    "    df_final = df_final.withColumn(\n",
    "        \"id\",\n",
    "        sha1(\n",
    "            concat_ws(\n",
    "                \"-\", \n",
    "                col(\"veiculo_letreiro_completo\"),\n",
    "                col(\"veiculo_linha_codigo\"),\n",
    "                col(\"veiculo_prefixo\"),\n",
    "                col(\"veiculo_horario_local_captura\"),\n",
    "                col(\"veiculo_latitude\"),\n",
    "                col(\"veiculo_longitude\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "        # Obtém o timestamp atual\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Imprime o timestamp com a mensagem\n",
    "    print(f\"{timestamp}: define transformation\")\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "def send_to_trusted(df_final):\n",
    "    # Supondo que df_final já está definido e contém uma coluna 'id' e 'veiculo_horario_referencia'\n",
    "\n",
    "    # Remover registros duplicados com base no campo 'id'\n",
    "    df_final_unique = df_final.dropDuplicates([\"id\"])\n",
    "\n",
    "    # Adiciona uma coluna com o formato 'yyyy-MM-dd-HH' baseado em veiculo_horario_referencia\n",
    "    df_final_unique = df_final_unique.withColumn(\n",
    "        \"datepartition\",\n",
    "        F.date_format(F.col(\"veiculo_horario_local_captura\"), \"yyyy-MM-dd-HH\")\n",
    "    )\n",
    "\n",
    "    # Ajuste o número de partições, se necessário\n",
    "    #df_final_unique = df_final_unique.repartition(4, \"datepartition\")\n",
    "\n",
    "    # Escreve o stream em CSV particionado por 'yyyy-MM-dd-HH', onde novos dados serão continuamente adicionados\n",
    "    df_final_unique.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"path\", trusted) \\\n",
    "        .option(\"checkpointLocation\", checkpoint) \\\n",
    "        .partitionBy(\"datepartition\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtém o timestamp atual\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Imprime o timestamp com a mensagem\n",
    "    print(f\"{timestamp}: escreveu os dados na trusted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw():\n",
    "    # Leitura incremental de arquivos JSON da camada raw\n",
    "    df_read = spark.read.format(\"json\").schema(schema).load(raw + f'/datepartition={data_inicio_processamento}')\n",
    "   \n",
    "    # Obtém o timestamp atual\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Imprime o timestamp com a mensagem\n",
    "    print(f\"{timestamp}: print carregou os dados da raw\")\n",
    "\n",
    "    #df_read.show()\n",
    "    return df_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import schedule\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"schedule\"])\n",
    "    import schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:41:52: print carregou os dados da raw\n",
      "2024-09-24 21:41:52: define transformation\n",
      "2024-09-24 21:46:35: escreveu os dados na trusted\n",
      "2024-09-24 21:46:35: print carregou os dados da raw\n",
      "2024-09-24 21:46:35: define transformation\n",
      "2024-09-24 21:51:14: escreveu os dados na trusted\n",
      "2024-09-24 21:51:14: print carregou os dados da raw\n",
      "2024-09-24 21:51:14: define transformation\n",
      "2024-09-24 21:56:08: escreveu os dados na trusted\n",
      "2024-09-24 21:56:09: print carregou os dados da raw\n",
      "2024-09-24 21:56:09: define transformation\n",
      "2024-09-24 22:00:50: escreveu os dados na trusted\n",
      "2024-09-24 22:00:51: print carregou os dados da raw\n",
      "2024-09-24 22:00:51: define transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m schedule\u001b[38;5;241m.\u001b[39mevery(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mminutes\u001b[38;5;241m.\u001b[39mdo(job)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mschedule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/schedule/__init__.py:854\u001b[0m, in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pending\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls :meth:`run_pending <Scheduler.run_pending>` on the\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    :data:`default scheduler instance <default_scheduler>`.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[43mdefault_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/schedule/__init__.py:101\u001b[0m, in \u001b[0;36mScheduler.run_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m runnable_jobs \u001b[38;5;241m=\u001b[39m (job \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mshould_run)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(runnable_jobs):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/schedule/__init__.py:173\u001b[0m, in \u001b[0;36mScheduler._run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, CancelJob) \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m CancelJob:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_job(job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/schedule/__init__.py:691\u001b[0m, in \u001b[0;36mJob.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CancelJob\n\u001b[1;32m    690\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_run \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_next_run()\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mjob\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjob\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[43msend_to_trusted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 27\u001b[0m, in \u001b[0;36msend_to_trusted\u001b[0;34m(df_final)\u001b[0m\n\u001b[1;32m     11\u001b[0m df_final_unique \u001b[38;5;241m=\u001b[39m df_final_unique\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatepartition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     F\u001b[38;5;241m.\u001b[39mdate_format(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mveiculo_horario_local_captura\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd-HH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Ajuste o número de partições, se necessário\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#df_final_unique = df_final_unique.repartition(4, \"datepartition\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Escreve o stream em CSV particionado por 'yyyy-MM-dd-HH', onde novos dados serão continuamente adicionados\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[43mdf_final_unique\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrusted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatepartition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Obtém o timestamp atual\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def job():\n",
    "    send_to_trusted(transform_data(load_raw()))\n",
    "\n",
    "# Executa a função pela primeira vez imediatamente\n",
    "job()\n",
    "\n",
    "schedule.every(5).minutes.do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
